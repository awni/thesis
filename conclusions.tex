\chapter{Conclusions}

In this work we developed a framework for transcribing real-valued sequences
with high accuracy. This framework has two primary axes for improving the
generalization of the model. The first is the architecture of the model and the
second is the amount of annotated data. As we increase the complexity of the
model and the size of the dataset, efficient computation also becomes an
essential ingredient.

We have shown how to apply this framework for three applications:
\begin{itemize}
    \setlength{\itemindent}{-1.5em}
    \item[]{\bf Arrhythmia Detection:} We develop a model and dataset which can
        transcribe an ECG sequence into a sequence of arrhythmia predictions.
        The model's performance is comparable to that of board-certified
        cardiologists.  Guided by our framework, we arrive at an architecture
        which consists of 34 layers with residual connections and batch
        normalization to make the optimization tractable.  The dataset
        collected and annotated for the task contains more than 500 times the
        number of unique patients than previously studied corpora.

    \item[]{\bf Speech Transcription:} We develop a first-pass speech
        transcription model. By continuing to refine the model's architecture
        and grow the training datasets we arrive at the Deep Speech 2
        recognizer. This model has error rates comparable to that of human
        transcribers in some English and Mandarin speech domains. We create
        novel modules and learning strategies which improve the learning
        dynamics and generalization error of the model. We also develop
        a sophisticated data construction and augmentation pipeline in order to
        grow our dataset to unprecedentedly large sizes. Finally, in order to
        make the optimization and deployment of the model tractable, we develop
        several highly effective techniques for efficient computation.

    \item[]{\bf Keyword Spotting and Voice Activity Detection:} By approaching
        the problems of keyword spotting and voice activity detection in an
        end-to-end fashion, we are able to leverage most of the data and
        modeling techniques that we developed for the Deep Speech 2 model. With
        these techniques we develop a single model and a custom inference
        algorithm which is able to achieve high true positive rates at low false
        positive rates for both tasks. We deploy this model to a smart phone in
        real-time without any loss of accuracy.

\end{itemize}

Countless sequence transcription problems can benefit from the application of
these techniques. However, there remains substantial room to improve along many
fronts. As hardware and software for computing these models continues to
improve rapidly, the key challenge will be our ability to learn models which
generalize well. Collecting large annotated datasets is time-consuming, costly
and in some cases impractical. Sample efficient methods which can still
generalize well will make the application of these models much more accessible.

Nevertheless, the take home message of this work is that with sufficient
resources we can achieve high accuracy on a wide range of sequence
transcription problems. While not all problems can be solved this way, problems
which do not require substantial high-level reasoning are typically amenable.
For these problems we can leverage large annotated datasets, efficient
computation and carefully tuned end-to-end models to match the performance of
human experts on challenging tasks.
