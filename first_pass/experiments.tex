\section{Experiments}

We evaluate our approach on the 81 hour Wall Street Journal (WSJ) news article
dictation corpus (available in the LDC catalog as LDC94S13B and LDC93S6B). Our
training set consists of 81 hours of speech from 37,318 utterances. The basic
preparation of transforming the LDC-released corpora into training and test
subsets follows the Kaldi speech recognition toolkit's s5 recipe
\cite{povey2011}. However, we did not apply much of the text normalization
used to prepare transcripts for training an HMM system. Instead we simply drop
unnecessary transcript notes like lexical stress, keeping transcribed word
fragments and acronym punctuation marks. We can safely discard much of this
normalization because our approach does not rely on a lexicon or pronunciation
dictionary, which cause problems especially for word fragments. Our language
models are the standard models released with the WSJ corpus without lexical
expansion. We used the `dev93' evaluation subset as a development set and
report final test set performance on the `eval92' evaluation subset. Both
subsets use the same 20k word vocabulary. The language model used for decoding
is constrained to this same 20k word vocabulary.

The input audio was converted into log-Mel filterbank features with 23
frequency bins. A context window of +/- 10 frames were concatenated to form a
final input vector of size 483. We did not perform additional feature
preprocessing or feature-space speaker adaptation.  Our output alphabet
consists of 32 classes, namely the blank symbol ``\_'', 26 letters, 3
punctuation marks (apostrophe, ., and -) as well as tokens for noise and space. 

\subsection{First-Pass Decoding with a Language Model}

We trained a BRDNN with 5 hidden layers, all with 1824 hidden units, for a
total of 20.9M free parameters. The third hidden layer of the network has
recurrent connections. Weights in the network are initialized from a uniform
random distribution scaled by the weight matrix's input and output layer size
\cite{glorot2011}. We use the Nesterov accelerated gradient optimization
algorithm \cite{sutskever2013} with initial learning rate $10^{-5}$, and
maximum momentum 0.95. After each full pass through the training set we divide
the learning rate by 1.2 to ensure the overall learning rate decreases over
time. We train the network for a total of 20 passes over the training set,
which takes about 96 hours using our Python GPU implementation.  For decoding
with prefix search we use a beam size of 200 and cross-validate with a held-out
set to find a good setting of the parameters $\alpha$ and $\beta$.
Table~\ref{tab:first_pass:res_decode} shows word and character error rates for
multiple approaches to decoding with this trained BRDNN.

\begin{table}
\centering
\begin{tabular}{lrr}
\toprule
Model & CER & WER\\
\midrule
No LM         & 10.0 & 35.8 \\
Dictionary LM & 8.5  & 24.4 \\
Bigram LM     & 5.7  & 14.1 \\
\bottomrule
\end{tabular}
\caption{Word error rate (WER) and character error rate (CER) results
  for a BDRNN trained with the CTC loss function.}
\label{tab:first_pass:res_decode}
\end{table}

Without any sort of language constraint WER is quite high, despite the fairly
low CER. This is consistent with our observation that many mistakes at the
character level occur when a word appears mostly correct but does not conform
to the highly irregular orthography of English. Prefix-search decoding using
the 20k word vocabulary as a prior over possible character sequences results in
a substantial WER improvement, but changes the CER relatively little. Comparing
the CERs of the no LM and dictionary LM approaches again demonstrates that
without an LM the characters are mostly correct but are distributed across many
words which increases WER. A large relative drop in both CER and WER occur when
we decode with a bigram LM. Performance of the bigram LM model demonstrates
that CTC-trained systems can attain competitive error rates without relying on
a lattice or n-best list generated by an existing speech system.

\subsection{The Effect of Recurrent Connections}

Previous experiments with DNN-HMM systems found minimal benefits from recurrent
connections in DNN acoustic models. It is natural to wonder whether recurrence,
and especially bi-directional recurrence, is an essential aspect of our
architecture. To evaluate the impact of recurrent connections we compare the
train and test CERs of DNN, RDNN, and BRDNN models while roughly controlling
for the total number of free parameters in the model.
Table~\ref{tab:first_pass:res_recurrence} shows the results for each type of
architecture. 

\begin{table}
\centering
\begin{tabular}{lrrr}
\toprule
Model & Parameters (M) & Train CER & Test CER \\
\midrule
DNN   & 16.8 & 3.8 & 22.3 \\
RDNN  & 22.0 & 4.2 & 13.5 \\
BRDNN & 20.9 & 2.8 & 10.7 \\
\bottomrule
\end{tabular}
\caption{Train and test set CER results for a DNN without recurrence, an RDNN,
         and a BRDNN.}
\label{tab:first_pass:res_recurrence}
\end{table}

Both variants of recurrent models show substantial test set CER improvements
over the non-recurrent DNN model. Note that we report performance for a DNN of
only 16.8M total parameters which is smaller than the total number of
parameters used in both the RDNN and BRDNN models. We found that larger DNNs
performed worse on the test set, suggesting that DNNs may be more prone to
over-fitting for this task. Although the BRDNN has fewer parameters than the
RDNN it performs better on both the training and test sets. Again this suggests
that the architecture itself drives improved performance rather than the total
number of free parameters. Conversely, because the gap between bi-directional
recurrence and single recurrence is small relative to a non-recurrent DNN,
on-line speech recognition using a singly recurrent network may be feasible
without overly damaging performance.
