\section{Introduction}
\label{sec:first_pass:introduction}

Modern large vocabulary continuous speech recognition (LVCSR) systems are
complex and difficult to modify. Much of this complexity stems from the
paradigm of modeling words as sequences of sub-phonetic states with hidden
Markov models (HMMs). HMM-based systems require carefully-designed training
recipes to construct consecutively more complex HMM recognizers. The overall
difficulty of building, understanding, and modifying HMM-based LVCSR systems
has limited progress in speech recognition and isolated it from many advances
in related fields. 

Previous work demonstrated an HMM-free approach for training a speech
recognizer~\cite{graves2014}. The authors use a neural network to directly
predict transcript characters given the audio of an utterance. This approach
discards many of the assumptions present in modern HMM-based LVCSR systems in
favor of treating speech recognition as a direct sequence transduction problem.
The approach trains a neural network using the connectionist temporal
classification (CTC) loss function, which amounts to maximizing the likelihood
of an output sequence by efficiently summing over all possible input-output
sequence alignments.  Using CTC the authors were able to train a neural network
to predict the character sequence of test utterances with a character error
rate (CER) under 10\% on the Wall Street Journal LVCSR corpus. While impressive
in its own right, these results are not yet competitive with existing HMM-based
systems in terms of word error rate (WER). Good word-level performance in
speech recognition often depends heavily upon a language model to provide a
prior probability over likely word sequences. 

To integrate language model information during decoding, the probabilities from
the CTC-trained neural network are used to rescore a lattice or n-best
hypothesis list generated by a state-of-the-art HMM-based
system~\cite{graves2014}. This introduces a potentially confounding factor
because an n-best list constrains the set of possible transcriptions
significantly.  Additionally, it results in an overall system which still
relies on HMM speech recognition infrastructure to achieve the final results.
In contrast, we present \emph{first-pass} decoding results which use a neural
network and language model to decode from scratch, rather than re-ranking an
existing set of hypotheses.

We describe a decoding algorithm which directly integrates a language model
with CTC-trained neural networks to search through the space of possible word
sequences. Our first-pass decoding algorithm enables CTC-trained models to
benefit from a language model without relying on an existing HMM-based system
to generate a word lattice. This removes the lingering dependence on
HMM-centric speech recognition toolkits and enables us to achieve fairly
competitive WER results with only a neural network and $n$-gram language model.

Deep neural networks (DNNs) are the most widely used neural network
architecture for speech recognition~\cite{hinton2012}. DNNs are a fairly
generic architecture for classification and regression problems. In HMM-based
LVCSR systems, DNNs act as acoustic models by predicting the HMM's hidden state
given the acoustic input for a point in time. However, in such HMM-DNN systems
the temporal reasoning about an output sequence takes place within the HMM
rather than the neural network. CTC training of neural networks forces the
network to model output sequence dependencies rather than reasoning about
single time frames independently from others. To better handle such temporal
dependencies previous work with CTC used long short term memory (LSTM)
networks. LSTM is a neural network architecture was originally designed to
prevent the vanishing gradient problem of sigmoidal DNNs or temporally
recurrent deep neural networks (RDNNs)~\cite{hochreiter1997}.

Our work uses RDNNs instead of LSTMs as a neural network architecture. RDNNs
are simpler overall, because there are only dense weight matrix connections
between subsequent layers. This simpler architecture is more amenable to
graphics processing unit (GPU) computing which can significantly reduce
training times. Recent work shows that with rectifier nonlinearities DNNs can
perform well in DNN-HMM systems without suffering from vanishing gradient
problems during optimization~\cite{dahl2013, zeiler2013, maas2013}. This makes
us hopeful that RDNNs with rectifier nonlinearities may be able to perform
comparably to LSTMs which are specially engineered to avoid vanishing
gradients.
