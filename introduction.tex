\chapter{Introduction}

Countless problems can be formulated as transcribing a real-valued sequence.
Some commonly studied examples include automatic speech recognition and
handwriting recognition. Reading electrocardiograms is another example from the
medical domain. In this work we develop a general framework for building high
accuracy models to transcribe real-valued sequences. Our framework requires two
critical ingredients: a differentiable model based on deep neural networks and
a sufficiently large annotated dataset. These ingredients can also be viewed as
our primary generalization knobs. The more resources we spend on finding an
accurate model and improving the size and quality of our annotated dataset, the
better the overall performance of the model. In many cases, we must bring to
bear techniques from high performance computing (HPC) to efficiently train
these models.

In this work we exclusively rely on models which fall under the classification
of Deep Learning. The advent of deep learning has dramatically improved the
state-of-the-art in many common machine learning benchmarks. Some examples
include classifying objects from images, language modeling, speech
transcription, speech-to-text, and machine translation. In some tasks, the
progress has been so remarkable that the performance of the models now matches
or exceeds that of human experts.

Deep learning works well for the task of transcribing real-valued sequences as
this framework falls into the regime of supervised learning. We put forth a
framework which, when utilized well, we believe can solve many sequence
transcription problems to near human expert levels. However, not all sequence
transcription problems can be solved to a high degree of accuracy with today's
algorithms and technology. Types of problems that can typically be solved with
these methods include ``perception" problems and more broadly speaking problems
which an expert human can do with a few seconds or less of
thought.\footnote{Classifying problems which can be solved by deep learning in
this way is due to Andrew Ng in e.g.
https://hbr.org/2016/11/what-artificial-intelligence-can-and-cant-do-right-now.}
Both speech transcription, key word spotting and detecting arrhythmias in
electrocardiograms meet this criteria.

Traditional speech recognition systems rely on many hand-engineered components
all brought together by the hidden markov model framework. Similarly, prior
work in arrhythmia detection also relies heavily on feature engineering and
other rule-based algorithms for beat and rhythm detection. While these models
and the assumptions they make can be useful to achieve generalization in
low-resource settings, they also introduce bias which is difficult to overcome
as we increase the available resources. They introduce a glass-ceiling on
performance.  Compared to these previous approaches for solving sequence
transcription problems, deep learning is much more {\it end-to-end}. Deep
learning based models can be trained end-to-end directly on the input and
output pairs in the dataset or with minimal pre and post processing. These
models are substantially more general and do not make many of the prior
assumptions that come along with the hand-engineered components of traditional
systems.

Since deep learning models are much more general, they can easily fail to
perform well on unseen data. They require two ingredients to achieve high
generalization. First we must carefully tune the architecture and other
constraints encoded in the model. Deep learning can be viewed as a framework
for {\it differentiable computing}. This framework consists of differentiable
modules (e.g. recurrent units, convolutional units, batch normalization, etc.)
and a recipe for connecting these modules and efficiently learning their
parameters.  The building blocks are able to introduce useful prior information
for the problem at hand while also maintaining flexibility since the model can
learn to overcome these biases during the learning process. However, finding
the right set of building blocks, sequencing them correctly and tuning their
hyperparameters can require considerable computational resources and careful
human effort. As we show in the following applications this effort can yield
dramatic improvements to the model's ability to learn and generalize well.

The second critical ingredient is the amount and quality of annotated data we
have available. Better models can compensate for less data and vice versa.
However, in the following applications we achieve the best performance by
jointly optimizing the two together. In the speech recognition applications, we
construct datasets as large as 12,000 hours. Prior work on publicly available
speech corpora typically use datasets in the 80-300 hour range. For the task of
arrhythmia detection, we develop a corpus which contains more than 500 times
the number of unique patients as the most commonly used publicly available
benchmark.

In order to efficiently iterate over model architectures on large annotated
datasets, we require considerable computation. The advent of GPUs in deep
learning has unlocked what otherwise would be a computationally intractable
problem. Even in the past few years, the performance of these models with
off-the-shelf hardware and software has improved multiple orders of magnitude.
Despite these dramatic improvements, in some cases considerable effort must
still be spent making these models efficient to train and deploy.  In
particular, the scale of the speech transcription problem we study is such that
data parallel optimization algorithms are essential. We develop and discuss
techniques for efficient computation on the GPU, efficient I/O and network
bandwidth utilization and efficient, low-latency deployments.

In summary, we develop end-to-end models for transcribing real-valued sequences
and discuss three applications of these models. The first is detecting abnormal
heart activity in electrocardiograms. The second is large vocabulary continuous
speech recognition and the third is keyword spotting and voice activity
detection. In all cases we show how to scale high capacity models to
unprecedentedly large datasets. With these techniques we can achieve
performance comparable to that of human experts and state-of-the-art error
rates in speech recognition for multiple languages.

\section{Contributions per Chapter}

We present the following contributions in the chapters of this dissertation:

\begin{itemize}
    \setlength{\itemindent}{3.4em}
    \item[{\bf Chapter 2:}]{\bf Background.} This chapter provides some
        background on the models used throughout this work including RNNs and
        CTC. We start with a brief introduction to RNNs including LSTMs and
        GRUs. We also discuss various optimization algorithms used for RNNs as
        well as gradient clipping. We follow this with an in-depth look at CTC.
        We explain the CTC loss function and how to efficiently compute it as
        well as how to perform inference with a CTC trained model. The next
        section discusses properties and limitations of CTC and we follow this
        by placing CTC in context with HMMs and encoder-decoder models. We also
        include a short section on practical advice when using CTC. At the end
        of the background section we give a short bibliography and references
        for further study of the related material. 

    \item [{\bf Chapter 3:}]{\bf Application to Arrhythmia Detection.} In
        Chapter 3 we develop an algorithm which exceeds the performance of
        board certified cardiologists in detecting a wide range of heart
        arrhythmias from electrocardiograms recorded with a single-lead
        wearable monitor. We build a dataset with more than 500 times the
        number of unique patients than previously studied corpora. On this
        dataset, we train a 34-layer convolutional neural network which maps a
        sequence of ECG samples to a sequence of rhythm classes. Committees of
        board-certified cardiologists annotate a gold standard test set on
        which we compare the performance of our model to that of 6 other
        individual cardiologists. For ten arrhythmias, Sinus rhythm and noise
        the model achieves performance comparable to that of expert
        cardiologists when using a committee consensus label as the ground
        truth.

    \item [{\bf Chapter 4:}]{\bf First-pass Speech Recognition.} Here we present
        a method to perform first-pass large vocabulary continuous speech
        recognition using only a neural network and language model. Deep neural
        network acoustic models are now commonplace in HMM-based speech
        recognition systems, but building such systems is a complex,
        domain-specific task. Prior work demonstrated the feasibility of
        discarding the HMM sequence modeling framework by directly predicting
        transcript text from audio. This chapter extends this approach in two
        ways. First, we demonstrate that a straightforward recurrent neural
        network architecture can achieve a high level of accuracy. Second, we
        propose and evaluate a modified prefix-search decoding algorithm. This
        approach to decoding enables first-pass speech recognition with a
        language model, completely unaided by the cumbersome infrastructure of
        HMM-based systems. Experiments on the Wall Street Journal corpus
        demonstrate fairly competitive word error rates, and the importance of
        bi-directional network recurrence. 

    \item [{\bf Chapter 5:}]{\bf Deep Speech -- Scaling Up End-to-end Speech
        Recognition.} In Chapter 5, we present a state-of-the-art speech
        recognition system developed using end-to-end deep learning. Our
        architecture is significantly simpler than traditional speech systems,
        which rely on laboriously engineered processing pipelines; these
        traditional systems also tend to perform poorly when used in noisy
        environments. In contrast, our system does not need hand-designed
        components to model background noise, reverberation, or speaker
        variation, but instead directly learns a function that is robust to
        such effects. We do not need a phoneme dictionary, nor even the concept
        of a "phoneme." Key to our approach is a well-optimized RNN training
        system that uses multiple GPUs, as well as a set of novel data
        synthesis techniques that allow us to efficiently obtain a large amount
        of varied data for training. Our system, called Deep Speech,
        outperforms previously published results on the widely studied
        Switchboard Hub5'00, achieving 16.0\% error on the full test set. Deep
        Speech also handles challenging noisy environments better than widely
        used, state-of-the-art commercial speech systems.

    \item [{\bf Chapter 6:}]{\bf Deep Speech 2 -- End-to-End Speech Recognition
        in English and Mandarin.} We show that an end-to-end deep learning
        approach can be used to recognize either English or Mandarin Chinese
        speechâ€“two vastly different languages. We show how to improve upon the
        Deep Speech model along three axis: the amount of annotated data, the
        architecture of the model and the efficiency of computation.  Th
        application of HPC techniques, enables experiments that previously took
        weeks to now run in days. This allows us to iterate more quickly to
        identify superior architectures and algorithms.  As a result, in
        several cases, our system is competitive with the transcription of
        human workers when benchmarked on standard datasets.  Finally, using a
        technique called Batch Dispatch with GPUs in the data center, we show
        that our system can be inexpensively deployed in an online setting,
        delivering low latency when serving users at scale.

    \item [{\bf Chapter 7:}]{\bf Keyword Spotting and Voice Activity
        Detection.} In this chapter we show how a single neural network
        architecture can be used for two tasks: on-line keyword spotting and
        voice activity detection. We develop novel inference algorithms for an
        end-to-end RNN trained with the CTC loss function which allow our model
        to achieve high accuracy on both keyword spotting and voice activity
        detection without retraining. In contrast to prior voice activity
        detection models, this architecture does not require aligned training
        data and uses the same parameters as the keyword spotting model. This
        allows us to deploy a high quality voice activity detector with no
        additional memory or maintenance requirements.

\end{itemize}

\section{First Published Appearances}
Most contributions described in this thesis have first appeared in various
publications. Much of the background material on CTC in Chapter 2 first
appeared in~\cite{rajpurkar2017}. The results of Chapter 3 first appeared in
~\cite{hannun2017sequence}. The results on first-pass speech recognition in
Chapter 4 were in~\cite{hannun2014firstpass}. Chapters 5 on Deep Speech and 6
on Deep Speech 2 were previously published in~\cite{hannun2014deepspeech}
and~\cite{amodei2016deep}. Finally, Chapter 7 on keyword Spotting and voice
activity detection was first published in~\cite{lengerich2016}.
